# Catapults in SGD and Adagrad

Ce dépôt a pour but d'explorer comment les catapultes en descente de gradient stochastique [1] améliorent la généralisation. Il contient aussi une étude et des preuves sur les catapultes avec Adagrad.

## Poster et rapport

Poster disponible [![ici](assets/poster_preview.png)](assets/Poster.pdf).

Rapport disponible [![ici](assets/poster_preview.png)](assets/Report.pdf).

## Références :

1. ZHU, Libin, LIU, Chaoyue, RADHAKRISHNAN, Adityanarayanan, et al. Catapults in sgd: spikes in the training loss and their impact on generalization through feature learning. arXiv preprint arXiv:2306.04815, 2023.